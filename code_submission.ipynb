{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonhuang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "import keras.backend as K\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, CuDNNLSTM, CuDNNGRU, Dense, Bidirectional, Embedding\n",
    "from keras.layers import concatenate, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import BatchNormalization, SpatialDropout1D, Dropout, Permute, Multiply, RepeatVector\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import Callback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Please note that this is the final version of the code, many works that we have done earlier for preprocessing had been removed for making our code more clear and easier to read.                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array for replace some contractions\n",
    "replace_patterns = [\n",
    "('(A|a)in\\'t', 'is not'),\n",
    "('(C|c)an\\'t', 'can not'),\n",
    "('(H|h)ow\\'s', 'how is'),\n",
    "('(H|h)ow\\'d', 'how did'), \n",
    "('(H|h)ow\\'d\\'y', 'how do you'),\n",
    "('(H|h)ere\\'s', 'here is'),\n",
    "('(I|i)t\\'s', 'it is'),\n",
    "('(I|i)\\'m', 'i am'),\n",
    "('f(u\\*|\\*c)k', 'fuck'),    \n",
    "('(L|l)et\\'s', 'let us'),\n",
    "('(M|m)a\\'am', 'madam'),\n",
    "('sh\\*t', 'shit'),\n",
    "('(S|s)han\\'t', 'shall not'), \n",
    "('(S|s)ha\\'n\\'t', 'shall not'), \n",
    "('(S|s)o\\'s', 'so as'),    \n",
    "('(T|t)his\\'s', 'this is'),\n",
    "('(T|t)here\\'s', 'there is'),\n",
    "('(W|w)on\\'t', 'will not'),\n",
    "('(W|w)hat\\'s', 'what is'),    \n",
    "('(W|w)hatis', 'what is'),\n",
    "('(W|w)hen\\'s', 'when is'), \n",
    "('(W|w)here\\'d', 'where did'), \n",
    "('(W|w)here\\'s', 'where is'), \n",
    "('(W|w)ho\\'s', 'who is'), \n",
    "('(W|w)hy\\'s', 'why is'),     \n",
    "('(Y|y)\\'all', 'you all'), \n",
    "('o\\'clock', 'of the clock'),\n",
    "('\\'cause', 'because'),  \n",
    "('(\\w+)\\'ve', '\\g<1> have'),\n",
    "('(\\w+)\\'ll', '\\g<1> will'),\n",
    "('(\\w+)n\\'t', '\\g<1> not'),\n",
    "('(\\w+)\\'re', '\\g<1> are'),\n",
    "('(\\w+)\\'d', '\\g<1> would')]\n",
    "# function for clean contractions\n",
    "def clean_contraction(x):    \n",
    "    x = str(x)    \n",
    "    for punct in \"’‘\":\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, \"'\") \n",
    "    for pattern, repl in replace_patterns:\n",
    "        if re.search(pattern,x):\n",
    "            x = re.sub(pattern, repl, x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# unify the expression of time\n",
    "check_pm = re.compile(r'[0-9]+p[.]?m[.]?')\n",
    "check_PM = re.compile(r'[0-9]+P[.]?M[.]?')\n",
    "check_am = re.compile(r'[0-9]+a[.]?m[.]?')\n",
    "check_AM = re.compile(r'[0-9]+A[.]?M$[.]?')\n",
    "# write the function \n",
    "def fix_time(x):  \n",
    "    x = str(x)\n",
    "    if re.search(check_pm, x):\n",
    "        x = re.sub('p.m', ' PM', x)\n",
    "    if re.search(check_PM, x):\n",
    "        x = re.sub('P.M', ' PM', x)\n",
    "    if re.search(check_am, x):\n",
    "        x = re.sub('a.m', ' AM', x)\n",
    "    if re.search(check_AM, x):\n",
    "        x = re.sub('A.M', ' AM', x)       \n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "# fix duplication of letters\n",
    "goood = re.compile(r'g+(o)\\1{2,}(d)+') # replace gooodddd by good\n",
    "check_duplicate = re.compile(r'\\w*(\\S)\\1{2,}\\w*') # replace words such as fantasticccccc by fantastic\n",
    "# fix duplications and clean some puncs\n",
    "def clean_punc(x):\n",
    "    x = str(x)\n",
    "    if re.search(goood,x): # we can treat goood and goooood in the same way\n",
    "        x = re.sub(goood, 'good', x)\n",
    "    if re.findall(check_duplicate,x): # we replace other duplicate characters\n",
    "        x = re.sub(r'(\\D)\\1{2,}', r'\\1', x)\n",
    "    if re.search('(\\[.*math).+(math\\])',x): # dealing with math functions(borrowed from kaggle)\n",
    "        x = re.sub('(\\[.*math).+(math\\])', '[latex formula]', x)\n",
    "    if \"'s \" in x:\n",
    "        x = x.replace(\"'s \",\" \")\n",
    "    if \"'\" in x:\n",
    "        x = x.replace(\"'\", '')\n",
    "    if \"_\" in x:\n",
    "        x = x.replace(\"_\", ' and ')\n",
    "    return x\n",
    "\n",
    "\n",
    "# we fix common wrong spellings in our specific document context\n",
    "mispell_dict = {    'colour':'color',\n",
    "                    'centre':'center',\n",
    "                    'didnt':'did not',\n",
    "                    'Didnt':'Did not',\n",
    "                    'Doesnt':'Does not',\n",
    "                    'Couldnt':'Could not',\n",
    "                    'doesnt':'does not',\n",
    "                    'isnt':'is not',\n",
    "                    'shouldnt':'should not',\n",
    "                    'flavour':'flavor',\n",
    "                    'flavours':'flavors',\n",
    "                    'wasnt':'was not',\n",
    "                    'cancelled':'canceled',\n",
    "                    'neighbourhood':'neighborhood',\n",
    "                    'neighbour':'neighbor',\n",
    "                    'theatre':'theater',\n",
    "                    'grey':'gray',\n",
    "                    'favourites':'favorites',\n",
    "                    'favourite':'favorite',\n",
    "                    'flavoured':'flavored',\n",
    "                    'acknowledgement':'acknowledgment',\n",
    "                    'judgement':'judgment',\n",
    "                    'speciality':'specialty',\n",
    "                    'favour':'favor',\n",
    "                    'colours':'colors',\n",
    "                    'coloured':'colored',\n",
    "                    'theatres':'theaters',\n",
    "                    'behaviour':'behavior',\n",
    "                    'travelling':'traveling',\n",
    "                    'colouring':'coloring',\n",
    "                    'labelled':'labeled',\n",
    "                    'cancelling':'canceling',\n",
    "                    'waitedand': 'waited and',\n",
    "                    'whisky':'Whisky',\n",
    "                    'tastey':'tasty',\n",
    "                    'goodbut': 'good but',\n",
    "                    'sushis':'sushi',\n",
    "                    'disapoointed': 'disappointed',\n",
    "                    'disapointed':'disappointed',\n",
    "                    'disapointment':'disappointment',\n",
    "                    'Amzing':'Amazing',\n",
    "                    'bAd':'bad',\n",
    "                    'fantastics':'fatastic',\n",
    "                    'flavuorful':'flavorful',\n",
    "                    'infomation':'information',\n",
    "                    'informaiton':'information',\n",
    "                    'eveeyone':'everyone',\n",
    "                    'Hsppy':'Happy',\n",
    "                    'waygu':'wagyu',\n",
    "                    'unflavorful':'untasty',\n",
    "                    'fiancÃ©':'fiance',\n",
    "                    'jalapeÃ±o':'jalapeno',\n",
    "                    'jalapeÃ±os':'jalapenos',\n",
    "                    'sautÃ©ed':'sauteed',\n",
    "                    'CafÃ©':'Cafe',\n",
    "                    'cafÃ©':'cafe',\n",
    "                    'entrÃ©e':'entree',\n",
    "                    'brÃ»lÃ©e':'brulee',\n",
    "                    'entrÃ©es':'entrees',\n",
    "                    'MontrÃ©al':'Montreal',\n",
    "                    'crÃ¨me':'creme',\n",
    "                    'JalapeÃ±o':'jalapeno',\n",
    "                    'crÃªpe':'crepe',\n",
    "                    'CrÃªpe':'Crepe',\n",
    "                    'Flavortown': 'Flavor Town',\n",
    "                    '\\u200b': ' ',\n",
    "                    'fck':'fuck',\n",
    "                    'wi-fi':'wifi',\n",
    "                    'ayce':'all you can eat',\n",
    "                    'appriceiate':'appriciate',\n",
    "                    'worest':'worst'}\n",
    "def correct_spelling(x):\n",
    "    x = str(x)\n",
    "    for word in mispell_dict.keys():\n",
    "        if word in x:\n",
    "            x = x.replace(word, mispell_dict[word])\n",
    "    return x\n",
    "\n",
    "# seperate words, numbers and some unremoved punctuations such as ,.?!\n",
    "def seperate_word(x):\n",
    "    for pattern, repl in [('[\\W]',lambda p:' '+p.group()+' '),('[0-9]{1,}',lambda p:' '+p.group()+' ')]:\n",
    "        if re.search(pattern,x):   \n",
    "            x = re.sub(pattern, repl, x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i   am   so   good ,    yolo .    iam   there   at    9    AM ,    pmont ,    lamb ! '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example of what above functions do\n",
    "x = 'i am so gooood, yolooo. iam there at 9a.m, pmont, lamb!'\n",
    "x = clean_contraction(x)\n",
    "x = clean_punc(x)\n",
    "x = fix_time(x)\n",
    "x = correct_spelling(x)\n",
    "x = seperate_word(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the train data and test data\n",
    "train_df = pd.read_csv('train_data.csv')\n",
    "label_df = pd.read_csv('train_label.csv')\n",
    "test_df = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply above defined function to complete the preprocessing\n",
    "# please note that we have to make the test data having the same look as train data for making predictions\n",
    "# however, we will not use anything from test data when train the model\n",
    "train_df['text'] = train_df['text'].str.lower().map(clean_contraction).map(clean_punc).map(fix_time).map(correct_spelling).map(seperate_word)\n",
    "test_df['text'] = test_df['text'].str.lower().map(clean_contraction).map(clean_punc).map(fix_time).map(correct_spelling).map(seperate_word)\n",
    "\n",
    "\n",
    "# ================================== END of preprocessing ==========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word to Index\n",
    "In this step, the vocab will be build and texts in reviews will be converted to corresponding index according to the order of words in vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting some the parameters\n",
    "maxlen = 280    # max number of words in each review\n",
    "max_words = 100000  # we only keep the most frequent 100000 words in vocab\n",
    "\n",
    "\n",
    "train_X = train_df[\"text\"].values   # put training data into an array\n",
    "test_X = test_df[\"text\"].values    # put testing data into an array\n",
    "tokenizer = Tokenizer(num_words=max_words, filters='\\t\\n\\r')\n",
    "tokenizer.fit_on_texts(list(train_X)) # tokenize the training data and build corpus\n",
    "\n",
    "\n",
    "# convert words to corresponding index according to the oder of words in vocab\n",
    "train_X = tokenizer.texts_to_sequences(train_X) \n",
    "test_X = tokenizer.texts_to_sequences(test_X) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101,\n",
       "  20,\n",
       "  38,\n",
       "  87,\n",
       "  36,\n",
       "  6,\n",
       "  888,\n",
       "  16,\n",
       "  106,\n",
       "  300,\n",
       "  1,\n",
       "  23,\n",
       "  47,\n",
       "  2,\n",
       "  508,\n",
       "  347,\n",
       "  5,\n",
       "  1327,\n",
       "  9,\n",
       "  39,\n",
       "  33,\n",
       "  7,\n",
       "  34,\n",
       "  669,\n",
       "  400,\n",
       "  42,\n",
       "  4,\n",
       "  2,\n",
       "  175,\n",
       "  26,\n",
       "  124,\n",
       "  1416,\n",
       "  168,\n",
       "  1573,\n",
       "  791,\n",
       "  669,\n",
       "  41,\n",
       "  30,\n",
       "  16,\n",
       "  461,\n",
       "  4,\n",
       "  5,\n",
       "  292,\n",
       "  6,\n",
       "  490,\n",
       "  89,\n",
       "  1,\n",
       "  382,\n",
       "  7,\n",
       "  38,\n",
       "  15,\n",
       "  100,\n",
       "  15,\n",
       "  168,\n",
       "  1573,\n",
       "  49,\n",
       "  67382,\n",
       "  14,\n",
       "  2096,\n",
       "  4147,\n",
       "  61,\n",
       "  424,\n",
       "  669,\n",
       "  32,\n",
       "  4455,\n",
       "  14,\n",
       "  747,\n",
       "  17,\n",
       "  21,\n",
       "  207,\n",
       "  474,\n",
       "  1,\n",
       "  260,\n",
       "  9,\n",
       "  8,\n",
       "  7,\n",
       "  148,\n",
       "  142,\n",
       "  19,\n",
       "  2,\n",
       "  379,\n",
       "  4,\n",
       "  2,\n",
       "  111,\n",
       "  474,\n",
       "  5,\n",
       "  27,\n",
       "  28,\n",
       "  132,\n",
       "  103,\n",
       "  381,\n",
       "  4,\n",
       "  738,\n",
       "  78,\n",
       "  323,\n",
       "  49,\n",
       "  2,\n",
       "  179,\n",
       "  24,\n",
       "  1031,\n",
       "  6,\n",
       "  274,\n",
       "  25,\n",
       "  14,\n",
       "  1,\n",
       "  2,\n",
       "  635,\n",
       "  25,\n",
       "  13,\n",
       "  258,\n",
       "  4,\n",
       "  692,\n",
       "  28,\n",
       "  102,\n",
       "  946,\n",
       "  76,\n",
       "  87,\n",
       "  24,\n",
       "  7,\n",
       "  2104,\n",
       "  795,\n",
       "  12,\n",
       "  317,\n",
       "  924,\n",
       "  16,\n",
       "  461,\n",
       "  474,\n",
       "  28,\n",
       "  7,\n",
       "  117,\n",
       "  189,\n",
       "  120,\n",
       "  190,\n",
       "  1,\n",
       "  2,\n",
       "  960,\n",
       "  2110,\n",
       "  28,\n",
       "  10,\n",
       "  2,\n",
       "  2806,\n",
       "  413],\n",
       " [7,\n",
       "  46,\n",
       "  34,\n",
       "  1496,\n",
       "  109,\n",
       "  23,\n",
       "  336,\n",
       "  32,\n",
       "  1,\n",
       "  5,\n",
       "  218,\n",
       "  2,\n",
       "  111,\n",
       "  4452,\n",
       "  3,\n",
       "  76,\n",
       "  8,\n",
       "  895,\n",
       "  4,\n",
       "  1359,\n",
       "  1,\n",
       "  2,\n",
       "  258,\n",
       "  8,\n",
       "  46,\n",
       "  768,\n",
       "  1,\n",
       "  90,\n",
       "  218,\n",
       "  2,\n",
       "  4471,\n",
       "  3,\n",
       "  4,\n",
       "  17,\n",
       "  28,\n",
       "  2,\n",
       "  192,\n",
       "  207,\n",
       "  5,\n",
       "  21,\n",
       "  176,\n",
       "  27,\n",
       "  22,\n",
       "  2,\n",
       "  265,\n",
       "  8,\n",
       "  101,\n",
       "  26,\n",
       "  2076,\n",
       "  3,\n",
       "  107,\n",
       "  59,\n",
       "  23,\n",
       "  2,\n",
       "  363,\n",
       "  25,\n",
       "  309,\n",
       "  1,\n",
       "  703,\n",
       "  23,\n",
       "  1356,\n",
       "  4,\n",
       "  973,\n",
       "  3,\n",
       "  186,\n",
       "  749,\n",
       "  5287,\n",
       "  23,\n",
       "  2,\n",
       "  1468,\n",
       "  348,\n",
       "  1,\n",
       "  16,\n",
       "  78,\n",
       "  1058,\n",
       "  15,\n",
       "  17,\n",
       "  29,\n",
       "  7,\n",
       "  189,\n",
       "  462,\n",
       "  14,\n",
       "  53,\n",
       "  3,\n",
       "  19,\n",
       "  260,\n",
       "  9,\n",
       "  8,\n",
       "  80,\n",
       "  35,\n",
       "  28,\n",
       "  78,\n",
       "  57,\n",
       "  331,\n",
       "  23,\n",
       "  7,\n",
       "  295,\n",
       "  193,\n",
       "  12,\n",
       "  383,\n",
       "  2,\n",
       "  165,\n",
       "  5,\n",
       "  107,\n",
       "  1,\n",
       "  19,\n",
       "  68,\n",
       "  8,\n",
       "  145,\n",
       "  4,\n",
       "  307,\n",
       "  86,\n",
       "  41],\n",
       " [1085,\n",
       "  720,\n",
       "  346,\n",
       "  3,\n",
       "  3258,\n",
       "  720,\n",
       "  346,\n",
       "  3,\n",
       "  508,\n",
       "  720,\n",
       "  346,\n",
       "  25,\n",
       "  3668,\n",
       "  1,\n",
       "  51,\n",
       "  683,\n",
       "  8,\n",
       "  2053,\n",
       "  4127,\n",
       "  4,\n",
       "  65,\n",
       "  57,\n",
       "  11,\n",
       "  3366,\n",
       "  2,\n",
       "  380,\n",
       "  1,\n",
       "  1470],\n",
       " [43,\n",
       "  5,\n",
       "  92,\n",
       "  155,\n",
       "  1181,\n",
       "  217,\n",
       "  5,\n",
       "  39,\n",
       "  1,\n",
       "  58,\n",
       "  18,\n",
       "  394,\n",
       "  14,\n",
       "  20,\n",
       "  238,\n",
       "  531,\n",
       "  143,\n",
       "  43,\n",
       "  18,\n",
       "  362,\n",
       "  405,\n",
       "  1,\n",
       "  1,\n",
       "  18,\n",
       "  113,\n",
       "  18,\n",
       "  203,\n",
       "  7,\n",
       "  158,\n",
       "  380,\n",
       "  4,\n",
       "  203,\n",
       "  6,\n",
       "  268,\n",
       "  31,\n",
       "  2,\n",
       "  13500,\n",
       "  1,\n",
       "  68,\n",
       "  394,\n",
       "  244,\n",
       "  1,\n",
       "  101,\n",
       "  219,\n",
       "  737,\n",
       "  13,\n",
       "  1539,\n",
       "  79,\n",
       "  45,\n",
       "  1,\n",
       "  30,\n",
       "  18,\n",
       "  298,\n",
       "  4,\n",
       "  292,\n",
       "  16,\n",
       "  360,\n",
       "  203,\n",
       "  2,\n",
       "  3460,\n",
       "  412,\n",
       "  500,\n",
       "  1,\n",
       "  18,\n",
       "  136,\n",
       "  112,\n",
       "  4,\n",
       "  68,\n",
       "  107,\n",
       "  103,\n",
       "  6,\n",
       "  112,\n",
       "  1927,\n",
       "  4,\n",
       "  18,\n",
       "  303,\n",
       "  112,\n",
       "  2,\n",
       "  380,\n",
       "  662,\n",
       "  1,\n",
       "  68,\n",
       "  107,\n",
       "  208,\n",
       "  2,\n",
       "  64,\n",
       "  4,\n",
       "  2170,\n",
       "  7,\n",
       "  380,\n",
       "  1,\n",
       "  52,\n",
       "  10,\n",
       "  88,\n",
       "  262,\n",
       "  73,\n",
       "  11203,\n",
       "  18,\n",
       "  203,\n",
       "  1,\n",
       "  68,\n",
       "  52,\n",
       "  10,\n",
       "  1304,\n",
       "  2,\n",
       "  168,\n",
       "  7276,\n",
       "  4,\n",
       "  73,\n",
       "  186,\n",
       "  585,\n",
       "  1,\n",
       "  18,\n",
       "  143,\n",
       "  43,\n",
       "  18,\n",
       "  92,\n",
       "  274,\n",
       "  182,\n",
       "  13011,\n",
       "  542,\n",
       "  14,\n",
       "  2,\n",
       "  158,\n",
       "  380,\n",
       "  4,\n",
       "  235,\n",
       "  52,\n",
       "  68,\n",
       "  274,\n",
       "  7,\n",
       "  158,\n",
       "  57,\n",
       "  14,\n",
       "  35,\n",
       "  1,\n",
       "  1,\n",
       "  68,\n",
       "  52,\n",
       "  10,\n",
       "  1094,\n",
       "  1,\n",
       "  729,\n",
       "  9,\n",
       "  720,\n",
       "  74,\n",
       "  1596,\n",
       "  152,\n",
       "  4,\n",
       "  74,\n",
       "  261,\n",
       "  7,\n",
       "  686,\n",
       "  1,\n",
       "  1,\n",
       "  68,\n",
       "  2211,\n",
       "  79,\n",
       "  132,\n",
       "  83,\n",
       "  4,\n",
       "  1890,\n",
       "  45,\n",
       "  35,\n",
       "  11,\n",
       "  7,\n",
       "  74,\n",
       "  302,\n",
       "  2092,\n",
       "  498,\n",
       "  1,\n",
       "  1,\n",
       "  101,\n",
       "  7970,\n",
       "  737,\n",
       "  531,\n",
       "  13,\n",
       "  1125,\n",
       "  79,\n",
       "  1,\n",
       "  1,\n",
       "  10,\n",
       "  1,\n",
       "  18,\n",
       "  27,\n",
       "  6,\n",
       "  262,\n",
       "  112,\n",
       "  43,\n",
       "  9,\n",
       "  8,\n",
       "  2,\n",
       "  159,\n",
       "  380,\n",
       "  1,\n",
       "  68,\n",
       "  143,\n",
       "  43,\n",
       "  18,\n",
       "  203,\n",
       "  978,\n",
       "  4,\n",
       "  18,\n",
       "  113,\n",
       "  65,\n",
       "  4,\n",
       "  68,\n",
       "  113,\n",
       "  101,\n",
       "  1002,\n",
       "  81,\n",
       "  282,\n",
       "  380,\n",
       "  11,\n",
       "  5801,\n",
       "  60,\n",
       "  29,\n",
       "  24,\n",
       "  167,\n",
       "  85,\n",
       "  49,\n",
       "  75,\n",
       "  44,\n",
       "  24,\n",
       "  220,\n",
       "  6,\n",
       "  33,\n",
       "  7,\n",
       "  5068,\n",
       "  4,\n",
       "  135,\n",
       "  1722,\n",
       "  8391,\n",
       "  1,\n",
       "  30,\n",
       "  324,\n",
       "  18,\n",
       "  292,\n",
       "  2426,\n",
       "  65,\n",
       "  18,\n",
       "  52,\n",
       "  10,\n",
       "  276,\n",
       "  13,\n",
       "  255,\n",
       "  504,\n",
       "  30,\n",
       "  18,\n",
       "  50,\n",
       "  48,\n",
       "  97,\n",
       "  9,\n",
       "  729,\n",
       "  1,\n",
       "  1,\n",
       "  68,\n",
       "  840,\n",
       "  101,\n",
       "  5,\n",
       "  727,\n",
       "  2,\n",
       "  833,\n",
       "  4,\n",
       "  274,\n",
       "  7,\n",
       "  13011,\n",
       "  542,\n",
       "  14,\n",
       "  1,\n",
       "  219,\n",
       "  448,\n",
       "  19,\n",
       "  24,\n",
       "  727,\n",
       "  15,\n",
       "  833,\n",
       "  132,\n",
       "  137,\n",
       "  18,\n",
       "  88,\n",
       "  136,\n",
       "  24,\n",
       "  73,\n",
       "  11203,\n",
       "  18,\n",
       "  203,\n",
       "  1,\n",
       "  73,\n",
       "  43,\n",
       "  18,\n",
       "  203,\n",
       "  21328,\n",
       "  11203,\n",
       "  4,\n",
       "  68,\n",
       "  2170,\n",
       "  7703,\n",
       "  1,\n",
       "  1,\n",
       "  717,\n",
       "  2,\n",
       "  414,\n",
       "  53,\n",
       "  18,\n",
       "  21,\n",
       "  176,\n",
       "  1204,\n",
       "  1,\n",
       "  73,\n",
       "  7,\n",
       "  18669,\n",
       "  7014,\n",
       "  5559,\n",
       "  1,\n",
       "  44,\n",
       "  10,\n",
       "  72,\n",
       "  6,\n",
       "  20,\n",
       "  173,\n",
       "  751,\n",
       "  24,\n",
       "  146,\n",
       "  6,\n",
       "  442,\n",
       "  23,\n",
       "  5535,\n",
       "  15,\n",
       "  135,\n",
       "  377,\n",
       "  8391,\n",
       "  4,\n",
       "  119,\n",
       "  6,\n",
       "  2142,\n",
       "  24,\n",
       "  121,\n",
       "  1,\n",
       "  18,\n",
       "  229,\n",
       "  4,\n",
       "  29,\n",
       "  48,\n",
       "  131,\n",
       "  6,\n",
       "  509,\n",
       "  9,\n",
       "  729,\n",
       "  63,\n",
       "  1343,\n",
       "  1,\n",
       "  3049,\n",
       "  1915,\n",
       "  1],\n",
       " [17,\n",
       "  21,\n",
       "  59,\n",
       "  32,\n",
       "  169,\n",
       "  150,\n",
       "  343,\n",
       "  53,\n",
       "  1,\n",
       "  218,\n",
       "  62,\n",
       "  5617,\n",
       "  1399,\n",
       "  2076,\n",
       "  4,\n",
       "  9,\n",
       "  643,\n",
       "  150,\n",
       "  2151,\n",
       "  1],\n",
       " [301,\n",
       "  22,\n",
       "  5,\n",
       "  94,\n",
       "  2,\n",
       "  1074,\n",
       "  1,\n",
       "  1812,\n",
       "  8861,\n",
       "  4,\n",
       "  1939,\n",
       "  3163,\n",
       "  1,\n",
       "  17,\n",
       "  52,\n",
       "  10,\n",
       "  21,\n",
       "  1939,\n",
       "  3163,\n",
       "  1,\n",
       "  30,\n",
       "  5,\n",
       "  277,\n",
       "  3,\n",
       "  17,\n",
       "  69,\n",
       "  10,\n",
       "  1126,\n",
       "  60,\n",
       "  25,\n",
       "  8861,\n",
       "  4,\n",
       "  1812,\n",
       "  22,\n",
       "  1717,\n",
       "  8,\n",
       "  332,\n",
       "  22,\n",
       "  265,\n",
       "  103,\n",
       "  381,\n",
       "  3,\n",
       "  8861,\n",
       "  8,\n",
       "  10,\n",
       "  34,\n",
       "  3,\n",
       "  90,\n",
       "  103,\n",
       "  381,\n",
       "  1,\n",
       "  30,\n",
       "  13,\n",
       "  657,\n",
       "  474,\n",
       "  4,\n",
       "  7,\n",
       "  9461,\n",
       "  6985,\n",
       "  86,\n",
       "  74,\n",
       "  1258,\n",
       "  22,\n",
       "  13,\n",
       "  7,\n",
       "  557,\n",
       "  873,\n",
       "  85,\n",
       "  85,\n",
       "  7605,\n",
       "  22,\n",
       "  105,\n",
       "  310,\n",
       "  54,\n",
       "  115,\n",
       "  22,\n",
       "  477,\n",
       "  3,\n",
       "  17,\n",
       "  498,\n",
       "  391,\n",
       "  43,\n",
       "  24,\n",
       "  146,\n",
       "  6,\n",
       "  55,\n",
       "  153,\n",
       "  704,\n",
       "  22,\n",
       "  2454,\n",
       "  86,\n",
       "  5,\n",
       "  66,\n",
       "  627,\n",
       "  3,\n",
       "  63,\n",
       "  1530,\n",
       "  4,\n",
       "  58,\n",
       "  9,\n",
       "  464,\n",
       "  6,\n",
       "  474,\n",
       "  3,\n",
       "  5,\n",
       "  129,\n",
       "  16,\n",
       "  537,\n",
       "  22,\n",
       "  22,\n",
       "  78,\n",
       "  57,\n",
       "  335,\n",
       "  1177,\n",
       "  17,\n",
       "  796,\n",
       "  40,\n",
       "  45452,\n",
       "  40,\n",
       "  6,\n",
       "  458,\n",
       "  1,\n",
       "  5,\n",
       "  39,\n",
       "  1709,\n",
       "  20,\n",
       "  38,\n",
       "  22],\n",
       " [34,\n",
       "  329,\n",
       "  12,\n",
       "  896,\n",
       "  4,\n",
       "  336,\n",
       "  2436,\n",
       "  3,\n",
       "  190,\n",
       "  2,\n",
       "  180,\n",
       "  5,\n",
       "  94,\n",
       "  52,\n",
       "  147,\n",
       "  7,\n",
       "  163,\n",
       "  6,\n",
       "  1259,\n",
       "  1,\n",
       "  634,\n",
       "  12,\n",
       "  483,\n",
       "  8,\n",
       "  123,\n",
       "  772,\n",
       "  37,\n",
       "  101,\n",
       "  1,\n",
       "  249,\n",
       "  3501,\n",
       "  29,\n",
       "  10797,\n",
       "  12,\n",
       "  1589,\n",
       "  76,\n",
       "  11,\n",
       "  99,\n",
       "  1],\n",
       " [5,\n",
       "  107,\n",
       "  14,\n",
       "  237,\n",
       "  13,\n",
       "  2,\n",
       "  38,\n",
       "  672,\n",
       "  2,\n",
       "  557,\n",
       "  3,\n",
       "  19,\n",
       "  92,\n",
       "  10,\n",
       "  200,\n",
       "  9,\n",
       "  4,\n",
       "  2833,\n",
       "  25,\n",
       "  284,\n",
       "  7,\n",
       "  852,\n",
       "  54,\n",
       "  1,\n",
       "  150,\n",
       "  652,\n",
       "  2279,\n",
       "  1693,\n",
       "  13,\n",
       "  168,\n",
       "  4,\n",
       "  234,\n",
       "  12,\n",
       "  852,\n",
       "  1,\n",
       "  849,\n",
       "  13,\n",
       "  2,\n",
       "  523,\n",
       "  15,\n",
       "  7,\n",
       "  489,\n",
       "  12,\n",
       "  902,\n",
       "  1900,\n",
       "  1616,\n",
       "  98,\n",
       "  14,\n",
       "  237,\n",
       "  13,\n",
       "  263,\n",
       "  35967,\n",
       "  1,\n",
       "  2,\n",
       "  489,\n",
       "  8,\n",
       "  450,\n",
       "  715,\n",
       "  4,\n",
       "  3787,\n",
       "  4,\n",
       "  92,\n",
       "  33,\n",
       "  629,\n",
       "  1290,\n",
       "  2,\n",
       "  988,\n",
       "  1,\n",
       "  2,\n",
       "  423,\n",
       "  8,\n",
       "  123,\n",
       "  34,\n",
       "  71,\n",
       "  2542,\n",
       "  84,\n",
       "  129,\n",
       "  15,\n",
       "  20,\n",
       "  8,\n",
       "  10,\n",
       "  15,\n",
       "  307,\n",
       "  12,\n",
       "  38,\n",
       "  3,\n",
       "  224,\n",
       "  3,\n",
       "  16,\n",
       "  6013,\n",
       "  52,\n",
       "  446,\n",
       "  14,\n",
       "  2,\n",
       "  820,\n",
       "  12,\n",
       "  16,\n",
       "  852,\n",
       "  6,\n",
       "  735,\n",
       "  6,\n",
       "  84,\n",
       "  1,\n",
       "  9,\n",
       "  350,\n",
       "  49,\n",
       "  68,\n",
       "  8,\n",
       "  361,\n",
       "  6,\n",
       "  33,\n",
       "  83,\n",
       "  1067,\n",
       "  23,\n",
       "  84,\n",
       "  6,\n",
       "  33,\n",
       "  4831,\n",
       "  58,\n",
       "  17,\n",
       "  28,\n",
       "  10,\n",
       "  284,\n",
       "  2,\n",
       "  2377,\n",
       "  63,\n",
       "  2,\n",
       "  423,\n",
       "  1,\n",
       "  5,\n",
       "  39,\n",
       "  21,\n",
       "  516,\n",
       "  13,\n",
       "  16,\n",
       "  6013,\n",
       "  4,\n",
       "  2,\n",
       "  423,\n",
       "  6,\n",
       "  21,\n",
       "  83,\n",
       "  2100,\n",
       "  13,\n",
       "  2,\n",
       "  2361,\n",
       "  640,\n",
       "  181,\n",
       "  4,\n",
       "  262,\n",
       "  13,\n",
       "  2,\n",
       "  7099,\n",
       "  489,\n",
       "  6,\n",
       "  1622,\n",
       "  62,\n",
       "  8957,\n",
       "  61,\n",
       "  72,\n",
       "  903,\n",
       "  23,\n",
       "  84,\n",
       "  404,\n",
       "  1],\n",
       " [14,\n",
       "  16,\n",
       "  5748,\n",
       "  13,\n",
       "  3430,\n",
       "  156,\n",
       "  2,\n",
       "  956,\n",
       "  3,\n",
       "  20,\n",
       "  11,\n",
       "  2,\n",
       "  127,\n",
       "  3430,\n",
       "  5,\n",
       "  21,\n",
       "  27,\n",
       "  171,\n",
       "  5,\n",
       "  229,\n",
       "  25553,\n",
       "  14,\n",
       "  19145,\n",
       "  22,\n",
       "  20342,\n",
       "  60,\n",
       "  1117,\n",
       "  3430,\n",
       "  348,\n",
       "  11,\n",
       "  1226,\n",
       "  22,\n",
       "  512,\n",
       "  17,\n",
       "  27,\n",
       "  1399,\n",
       "  6290,\n",
       "  4,\n",
       "  2,\n",
       "  878,\n",
       "  650,\n",
       "  28,\n",
       "  83,\n",
       "  4975,\n",
       "  4,\n",
       "  27,\n",
       "  14714,\n",
       "  26,\n",
       "  15,\n",
       "  39,\n",
       "  21,\n",
       "  501,\n",
       "  84,\n",
       "  67,\n",
       "  391,\n",
       "  335,\n",
       "  61,\n",
       "  83,\n",
       "  1,\n",
       "  1027,\n",
       "  26,\n",
       "  2495,\n",
       "  29,\n",
       "  46,\n",
       "  145,\n",
       "  4,\n",
       "  2,\n",
       "  38,\n",
       "  11,\n",
       "  253,\n",
       "  4,\n",
       "  99,\n",
       "  26,\n",
       "  13,\n",
       "  7,\n",
       "  3430,\n",
       "  435,\n",
       "  1,\n",
       "  39,\n",
       "  72,\n",
       "  64,\n",
       "  43,\n",
       "  156,\n",
       "  2,\n",
       "  197,\n",
       "  1],\n",
       " [46,\n",
       "  964,\n",
       "  134,\n",
       "  1071,\n",
       "  6,\n",
       "  532,\n",
       "  379,\n",
       "  800,\n",
       "  1876,\n",
       "  451,\n",
       "  4,\n",
       "  109,\n",
       "  8,\n",
       "  75,\n",
       "  651,\n",
       "  1,\n",
       "  1186,\n",
       "  22,\n",
       "  105,\n",
       "  131,\n",
       "  115,\n",
       "  1]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the tfidf features\n",
    "In this step, we are going to extract the tfidf features. We will build the bag of word model so that we can calculate the word frequency and inverse document frequency. Please note that the test data has not been used in building a vocab but we also need to extract tfidf features for test data as well. Ensure the test data has the same features with training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================= tfidf ========================================================\n",
    "\n",
    "# tranform the vocab dictionary to {index:word} format\n",
    "id_to_word = {}\n",
    "for word in tokenizer.word_index:\n",
    "    id_to_word.update({tokenizer.word_index[word]:word})\n",
    "\n",
    "# using the function from genism to build a vocab dictionary\n",
    "word_list = train_df[\"text\"].str.split()\n",
    "word_list2 = test_df[\"text\"].str.split()\n",
    "dictionary = corpora.Dictionary(word_list)\n",
    "\n",
    "# create the bag of word model looks like [[(0,f),(1,f),(2,f)],[(0,f),(1,f),(2,f)]] where f represents term frequency\n",
    "string_bow = list(word_list.map(dictionary.doc2bow)) + list(word_list2.map(dictionary.doc2bow)) # calculate term frequencies for each review\n",
    "\n",
    "# convert the word document co-occurrence matrix into tfidf matrix\n",
    "tfidf = models.TfidfModel(string_bow) \n",
    "\n",
    "# function for converting a tup to a dictionary\n",
    "def tup_to_dict(x):\n",
    "    dic = {}\n",
    "    for i in x:\n",
    "        dic.update({i[0]:i[1]}) \n",
    "    return dic\n",
    "\n",
    "# get the each review's id and corresponding tfidf vector in tuple pairs\n",
    "id_to_tfidf_tup = [tfidf[id_to_fre] for id_to_fre in string_bow]\n",
    "# convert the tuples to dict\n",
    "id_to_tfidf_dict = [tup_to_dict(i) for i in id_to_tfidf_tup]\n",
    "\n",
    "train_dict = id_to_tfidf_dict[:len(train_X)]\n",
    "test_dict = id_to_tfidf_dict[len(train_X):]\n",
    "\n",
    "\n",
    "word_to_id = dictionary.token2id  # convert tokenized word to corresponding index according to vocab\n",
    "\n",
    "# write a function to get corresponding index in tfidf dict for each token\n",
    "def get_id(token_id):\n",
    "    word = [id_to_word[i] for i in token_id] # converting token ids back to token words\n",
    "    # check whether these token words exist in the dictionary build for tfidf, replace by 0 if not exist\n",
    "    tfidf_id = [word_to_id.get(i) if word_to_id.get(i) is not None else 0 for i in word] \n",
    "    return tfidf_id    \n",
    "\n",
    "# apply above function\n",
    "# replace tokens in each reviews by corresponding id in the tfidf dict\n",
    "train_tfidf_id = [get_id(i) for i in train_X]\n",
    "test_tfidf_id = [get_id(i) for i in test_X]\n",
    "\n",
    "# a function to get corresponding tfidf value given token id\n",
    "def get_tfidf(id_tf,tid):\n",
    "    return[id_tf.get(i,0) for i in tid]\n",
    "\n",
    "# replace each token id by corresponding tfidf value for each review\n",
    "train_tfidf = [get_tfidf(train_dict[i],tid) for i,tid in enumerate(train_tfidf_id)]\n",
    "test_tfidf = [get_tfidf(test_dict[i],tid) for i,tid in enumerate(test_tfidf_id)]\n",
    "\n",
    "#========================================= END ===================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del id_to_word, word_list, dictionary, string_bow, tfidf, id_to_tfidf_tup, id_to_tfidf_dict, train_dict, test_dict, word_to_id, train_tfidf_id, test_tfidf_id\n",
    "# delete variables that will not be used further and collect memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the length of each review and padding those shorter sentences with 0s\n",
    "train_tfidf = pad_sequences(train_tfidf, maxlen=maxlen, dtype='float64')\n",
    "test_tfidf = pad_sequences(test_tfidf, maxlen=maxlen, dtype='float64')\n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "train_y = label_df['label'].values\n",
    "# Now all the features required had been extracted\n",
    "# the next step is getting the embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Getting embeddng matrix\n",
    "In this step, we will embed words in vocab based on 4 pre-trained embeddings and concatenate all 4 embedding vectors. As a result, each word can be represented by a 900 dimension dense vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pre-trained embedding files\n",
    "glove = 'glove.840B.300d.txt'\n",
    "paragram =  'paragram_300_sl999.txt'\n",
    "wiki_news = 'wiki-news-300d-1M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======================================= embedding ==============================================\n",
    "\n",
    "# function for loading embedding files\n",
    "def load_embed(file):\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')    \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding=\"utf8\", errors='ignore') if len(o)>100)   \n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "embedding_matrix = []\n",
    "for embed_path in [glove,paragram,wiki_news]:\n",
    "    embed = load_embed(embed_path)\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_words, len(word_index))\n",
    "    embedding_vec = np.zeros((nb_words, 300))    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_words: continue   # only consider the most 100000 frequent words\n",
    "            # embedding the word if it can be easily found in pre-trained file\n",
    "        if embed.get(word) is not None: \n",
    "            embedding_vec[i] = embed.get(word)\n",
    "            # embedding the word if the upper case version of it can be found\n",
    "        elif embed.get(word.upper()) is not None: \n",
    "            embedding_vec[i] = embed.get(word.upper())\n",
    "            # embedding the word if the capitalized version can be found\n",
    "        elif embed.get(word.capitalize()) is not None: \n",
    "            embedding_vec[i] = embed.get(word.capitalize())\n",
    "        else: # otherwise, we use the vector of the word 'something'\n",
    "            embedding_vec[i] = embed.get('something')        \n",
    "    embedding_matrix.append(embedding_vec)    \n",
    "    del embed\n",
    "    gc.collect()\n",
    "    \n",
    "# concate all 3 embedding vectors\n",
    "embedding = np.concatenate(embedding_matrix,axis=1)   \n",
    "del embedding_matrix\n",
    "gc.collect()\n",
    "\n",
    "# https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\n",
    "from gensim.models import KeyedVectors\n",
    "# load the 4th pre-trained embeddings (googleNews embeddings)\n",
    "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin'\n",
    "ggle = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "words = ggle.index2word\n",
    "embed = {}  # change ggle to a dictionary\n",
    "for i,word in enumerate(words):\n",
    "    embed[word] = i\n",
    "\n",
    "embedding_vec = np.zeros((nb_words, 300))    \n",
    "for word, i in word_index.items():\n",
    "    if i >= max_words: continue            \n",
    "    if embed.get(word) is not None: embedding_vec[i] = ggle[word]\n",
    "            # embedding the word if the upper case version of it can be found\n",
    "    elif embed.get(word.upper()) is not None: embedding_vec[i] = ggle[word.upper()]\n",
    "            # embedding the word if the capitalized version can be found\n",
    "    elif embed.get(word.capitalize()) is not None: embedding_vec[i] = ggle[word.capitalize()]\n",
    "    else: embedding_vec[i] = ggle['something']             \n",
    "        \n",
    "ggle_embed = embedding_vec\n",
    "\n",
    "# concate the preovious embedding vectors with the 4th one\n",
    "embedding = np.concatenate([embedding, ggle_embed], axis=1)\n",
    "\n",
    "#============================================== END =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving all required files and upload to colab for training through GPU\n",
    "# this step can be skipped if you do not need to train the model in a cloud platform\n",
    "np.savetxt('1200_embedding_280', embedding, delimiter=\",\")\n",
    "np.savetxt('train_x_280', train_X, delimiter=\",\")\n",
    "np.savetxt('train_tfidf_280', train_tfidf, delimiter=\",\")\n",
    "np.savetxt('train_y_280', train_y, delimiter=\",\")\n",
    "np.savetxt('test_tfidf_280', test_tfidf, delimiter=\",\")\n",
    "np.savetxt('test_x_280', test_X, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "Please note training this neural network model as following steps is time-consuming, GPU may be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are training the model in colab\n",
    "# following code is required to mount the jupyter notebook on your google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required libraries\n",
    "import re\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "import keras.backend as K\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, CuDNNLSTM, CuDNNGRU, Dense, Bidirectional, Embedding\n",
    "from keras.layers import concatenate, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import BatchNormalization, SpatialDropout1D, Dropout, Permute, Multiply, RepeatVector\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import Callback\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers as initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the cylical learning that we will use in our model\n",
    "# referenced from: https://github.com/anandsaha/pytorch.cyclic.learning.rate/blob/master/cls.py\n",
    "class CyclicLR(Callback):\n",
    "    def __init__(self, base_lr=0.0001, max_lr=0.002, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())                    \n",
    "    def on_batch_end(self, epoch, logs=None):        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# unzip the uploaded files in colab\n",
    "file_name = ['/content/a/My Drive/1200_280.zip']\n",
    "\n",
    "for file in file_name:\n",
    "    fz = zipfile.ZipFile(file, 'r')\n",
    "    for each in fz.namelist():\n",
    "        fz.extract(each, r'.')\n",
    "    fz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the uploaded and unziped files\n",
    "train_tfidf = np.loadtxt('/content/train_tfidf_280', delimiter = ',')\n",
    "train_y = np.loadtxt('/content/a/My Drive/train_y_280', delimiter=',')\n",
    "train_X = np.loadtxt('/content/train_x_280', delimiter=',')\n",
    "embedding = np.loadtxt('/content/900_embedding_280', delimiter=',')\n",
    "test_tfidf = np.loadtxt('/content/a/My Drive/test_tfidf_280', delimiter=',')\n",
    "test_X = np.loadtxt('/content/a/My Drive/test_x_280', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulate the label from 1-5 to 0-4\n",
    "train_y = train_y - 1\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutting the training data and label for validation\n",
    "train_x, val_x = train_X[:600000], train_X[600000:]\n",
    "train_idf, val_idf = train_tfidf[:600000], train_tfidf[600000:]\n",
    "train_y1, val_y = train_y[:600000], train_y[600000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the parameters\n",
    "maxlen=280\n",
    "max_words = 100000 # embedding size or vocab size\n",
    "EMBEDDING_DIM = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============ build the model ============\n",
    "word = Input(shape=(maxlen,))    \n",
    "embed = Embedding(max_words, EMBEDDING_DIM, trainable=False, input_length=maxlen, weights=[embedding])(word)\n",
    "\n",
    "tfidf = Input(shape=(maxlen,)) \n",
    "output2 = RepeatVector(900)(tfidf)  # repeating the tfidf input 900 times make=ing it the same dimension as embeddings\n",
    "output2 = Permute((2, 1), input_shape=(900, maxlen))(output2)  # switch the axis of matrix\n",
    "output2 = Multiply()([output2, embed]) # multiply embedding outputs and repeated tfidf\n",
    "output2 = GlobalMaxPooling1D()(output2) # pooling\n",
    "output2 = Dense(256, activation='relu')(output2) # pass it to a regular feed forward dense layer\n",
    "\n",
    "output1 = SpatialDropout1D(0.2)(embed)    \n",
    "output1 = Bidirectional(CuDNNLSTM(300, return_sequences=True))(output1)\n",
    "output1 = Bidirectional(CuDNNLSTM(300, return_sequences=True))(output1)\n",
    "avg_pool = GlobalAveragePooling1D()(output1)\n",
    "max_pool = GlobalMaxPooling1D()(output1)\n",
    "output1 = concatenate([avg_pool, max_pool])    \n",
    "output1 = BatchNormalization()(output1)\n",
    "output1 = Dense(256, activation='relu')(output1)\n",
    "output1 = concatenate([output1, output2])  # concate the output1(from LSTM) and output2(from tfidf)\n",
    "output1 = Dense(128, activation='relu')(output1) \n",
    "output1 = Dense(5, activation='softmax')(output1)\n",
    "\n",
    "model = Model(inputs=[word,tfidf], outputs=outputs) \n",
    "adam = optimizers.Adam(clipvalue=3.5) #Gradients will be clipped when their absolute value exceeds this value\n",
    "model.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=['acc'])    \n",
    "\n",
    "\n",
    "# fit your model\n",
    "clr = CyclicLR(base_lr=1e-4, max_lr=2e-3, step_size=np.ceil(1.5 * train_X.shape[0]/512))\n",
    "model.fit([train_X,train_tfidf], train_y, batch_size=512, epochs=3, verbose=1, callbacks=[clr])\n",
    "# validation_data = ([val_x,val_idf],val_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============== prediction and submission =================\n",
    "\n",
    "y_pred = model.predict([test_X, test_tfidf], batch_size = 512) # predict on test data\n",
    "y_pred = y_pred.argmax(axis=1).astype(int) \n",
    "y_pred = [x+1 for x in list(y_pred)]  # convert the label back to 1-5\n",
    "\n",
    "\n",
    "submission = pd.read_csv('/content/a/My Drive/test_data.csv') # read the test data\n",
    "submission['label'] = y_pred # insert with predicted label\n",
    "submission[['test_id','label']].to_csv('1200d_280.csv', index = False) # write the submission file\n",
    "\n",
    "#======================= END =============================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
